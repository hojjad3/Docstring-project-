{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install unsloth"
      ],
      "metadata": {
        "id": "GJbq265murYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "dmugHDQB6Ix8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "iE__Sq7Wu4cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, PatchDPOTrainer, is_bfloat16_supported\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "import torch\n",
        "import ast\n",
        "import re\n",
        "import textwrap\n",
        "import gc\n",
        "import os\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "NYH44dg0p7lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load And Bulid The Model"
      ],
      "metadata": {
        "id": "Q17TvAwWvXQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "model_name = \"unsloth/Qwen2.5-Coder-1.5B-Instruct\"\n",
        "\n",
        "PatchDPOTrainer()\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "HzDiJGR8qCXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load  Dataset"
      ],
      "metadata": {
        "id": "ZSiZPtDvv88N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file = \"/content/groq_dpo_dataset .jsonl\"\n",
        "\n",
        "print(f\"Loading Dataset from: {dataset_file}...\")\n",
        "\n",
        "def formatting_func(example):\n",
        "    return {\n",
        "        \"prompt\": example[\"prompt\"],\n",
        "        \"chosen\": example[\"chosen\"],\n",
        "        \"rejected\": example[\"rejected\"],\n",
        "    }\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=dataset_file, split=\"train\")\n",
        "\n",
        "original_columns = dataset.column_names\n",
        "dataset = dataset.map(formatting_func, remove_columns=original_columns)\n",
        "print(\" Dataset loaded successfully!\")"
      ],
      "metadata": {
        "id": "wLHtXDnmCBWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train The Model"
      ],
      "metadata": {
        "id": "TD-WBjGuwm_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 8,\n",
        "\n",
        "    warmup_ratio = 0.1,\n",
        "    num_train_epochs = 3,\n",
        "    learning_rate = 5e-6,\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 10,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    seed = 42,\n",
        "    output_dir = \"dpo_outputs\",\n",
        "\n",
        "    gradient_checkpointing = True,\n",
        "\n",
        "    beta = 0.1,\n",
        "    max_length = max_seq_length,\n",
        "    max_prompt_length = 1024,\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    ref_model = None,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = training_args,\n",
        ")\n",
        "\n",
        "print(\"Starting Training (Low Memory Mode)...\")\n",
        "dpo_trainer.train()"
      ],
      "metadata": {
        "id": "kR78GxWJq82a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test The Model"
      ],
      "metadata": {
        "id": "wcHeENbOwyQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_definitions(code_str):\n",
        "    names = []\n",
        "    try:\n",
        "        tree = ast.parse(code_str)\n",
        "        for node in tree.body:\n",
        "            if isinstance(node, ast.ClassDef):\n",
        "                names.append(node.name)\n",
        "            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n",
        "                if not node.name.startswith('_'):\n",
        "                    names.append(node.name)\n",
        "    except SyntaxError:\n",
        "        pass\n",
        "    return names"
      ],
      "metadata": {
        "id": "u6-XHupCr4P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_google_style_strict(docstring, node_type=None, function_name=\"\"):\n",
        "    errors = []\n",
        "    if not docstring:\n",
        "        return [\"Docstring is empty or missing.\"]\n",
        "\n",
        "    if \"Args:\" in docstring:\n",
        "        pattern = r\"\\s+[\\w\\*]+\\s*\\(.*\\):\\s+\"\n",
        "        if not re.search(pattern, docstring):\n",
        "            errors.append(\"Bad 'Args' format. Expected indentation + 'name (type): description'.\")\n",
        "\n",
        "    is_class = (node_type == ast.ClassDef)\n",
        "    is_init = (function_name == \"__init__\")\n",
        "\n",
        "    if not is_class and not is_init:\n",
        "        if \"Returns:\" not in docstring and \"Yields:\" not in docstring:\n",
        "            errors.append(\"Missing 'Returns:' (or 'Yields:') section.\")\n",
        "\n",
        "    return errors"
      ],
      "metadata": {
        "id": "qHyoMYgUr7Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_code_block(text):\n",
        "    pattern = r\"```(?:python)?\\s*(.*?)\\s*```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "HztQ0SG3rfDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "def generate_verified_docstring(model, tokenizer, code_snippet, max_retries=3):\n",
        "    real_names = extract_definitions(code_snippet)\n",
        "\n",
        "    system_instruction = \"\"\"You are a strict Python Code Editor.\n",
        "    Your goal: Rewrite the provided code to insert Google-Style docstrings.\n",
        "\n",
        "    ### CRITICAL RULES:\n",
        "    1. Output the **FULL COMPLETE CODE** (imports, classes, functions). Do NOT summarize.\n",
        "    2. Output ONLY valid Python code inside ```python``` blocks.\n",
        "    3. Format Args: `param_name (type): description`.\n",
        "    4. **Classes** do NOT need a 'Returns' section.\n",
        "    5. `__init__` does NOT need a 'Returns' section.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_instruction},\n",
        "        {\"role\": \"user\", \"content\": f\"Here is the code to document:\\n\\n{code_snippet}\"}\n",
        "    ]\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        print(f\"  Attempt {attempt + 1}...\", end=\" \")\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs,\n",
        "                max_new_tokens=2048,\n",
        "                do_sample=True,\n",
        "                temperature=0.2,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        generated_tokens = outputs[0][inputs.shape[-1]:]\n",
        "        full_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        generated_code = extract_code_block(full_output)\n",
        "\n",
        "        if not generated_code:\n",
        "            generated_code = full_output.strip()\n",
        "\n",
        "        errors = []\n",
        "\n",
        "        try:\n",
        "            module = ast.parse(generated_code)\n",
        "        except SyntaxError as e:\n",
        "            errors.append(f\"Syntax Error: {e}\")\n",
        "\n",
        "        if not errors:\n",
        "            found_names = []\n",
        "            for node in module.body:\n",
        "                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n",
        "                    found_names.append(node.name)\n",
        "                    if node.name.startswith('_') and node.name != \"__init__\": continue\n",
        "\n",
        "                    docstring_content = ast.get_docstring(node)\n",
        "\n",
        "                    style_errors = check_google_style_strict(\n",
        "                        docstring_content,\n",
        "                        node_type=type(node),\n",
        "                        function_name=node.name\n",
        "                    )\n",
        "\n",
        "                    if style_errors:\n",
        "                        errors.append(f\"In '{node.name}': {', '.join(style_errors)}\")\n",
        "\n",
        "            for name in real_names:\n",
        "                if name not in found_names:\n",
        "                    errors.append(f\"Missing original definition '{name}'. YOU MUST RETURN THE FULL CODE.\")\n",
        "\n",
        "        if not errors:\n",
        "            print(\"Success!\")\n",
        "            return generated_code\n",
        "        else:\n",
        "            print(f\"Failed: {errors[0]}...\")\n",
        "\n",
        "            error_msg = f\"Your code has errors: {errors}. \\nCRITICAL: You must rewrite the **ENTIRE ORIGINAL CODE** with fixes. Do not output snippet only.\"\n",
        "\n",
        "            messages.append({\"role\": \"assistant\", \"content\": full_output})\n",
        "            messages.append({\"role\": \"user\", \"content\": error_msg})\n",
        "\n",
        "    print(\"Max retries reached.\")\n",
        "    return generated_code"
      ],
      "metadata": {
        "id": "exE1YGHgro-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complex_test_code = \"\"\"\n",
        "def calculate_velocity(distance, time):\n",
        "    if time == 0: raise ValueError(\"Time cannot be zero\")\n",
        "    return distance / time\n",
        "\"\"\"\n",
        "meta_code = \"\"\"\n",
        "from typing import Any, Dict\n",
        "\n",
        "class SingletonMeta(type):\n",
        "    _instances: Dict[Any, Any] = {}\n",
        "\n",
        "    def __call__(cls, *args, **kwargs):\n",
        "        if cls not in cls._instances:\n",
        "            instance = super().__call__(*args, **kwargs)\n",
        "            cls._instances[cls] = instance\n",
        "        return cls._instances[cls]\n",
        "\n",
        "class Database(metaclass=SingletonMeta):\n",
        "    def __init__(self, connection_string: str):\n",
        "        self.connection_string = connection_string\n",
        "\"\"\"\n",
        "async_code = \"\"\"\n",
        "import asyncio\n",
        "\n",
        "class AsyncDatabaseConnection:\n",
        "    def __init__(self, db_url: str):\n",
        "        self.db_url = db_url\n",
        "        self.connection = None\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        print(f\"Connecting to {self.db_url}...\")\n",
        "        await asyncio.sleep(0.1)  # Simulate connection\n",
        "        self.connection = \"Connected\"\n",
        "        return self.connection\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc, tb):\n",
        "        print(\"Closing connection...\")\n",
        "        await asyncio.sleep(0.1)  # Simulate closing\n",
        "        self.connection = None\n",
        "\"\"\"\n",
        "dynamic_code = \"\"\"\n",
        "from typing import Any\n",
        "\n",
        "class LazyProxy:\n",
        "    def __init__(self, target: Any):\n",
        "        self._target = target\n",
        "\n",
        "    def __getattr__(self, name: str) -> Any:\n",
        "        print(f\"Intercepting access to {name}\")\n",
        "        attr = getattr(self._target, name)\n",
        "\n",
        "        if callable(attr):\n",
        "            def wrapper(*args, **kwargs):\n",
        "                print(f\"Calling method: {name}\")\n",
        "                return attr(*args, **kwargs)\n",
        "            return wrapper\n",
        "        return attr\n",
        "\"\"\"\n",
        "chatgpt = \"\"\"\n",
        "def normalize_text(text, lower=True, remove_punctuation=False):\n",
        "    if lower:\n",
        "        text = text.lower()\n",
        "\n",
        "    if remove_punctuation:\n",
        "        import string\n",
        "        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    return text.strip()\n",
        "    \"\"\"\n",
        "print(\"Starting Strict Verification...\\n\")\n",
        "print(generate_verified_docstring(model, tokenizer, complex_test_code))\n",
        "print(generate_verified_docstring(model, tokenizer, meta_code))\n",
        "print(generate_verified_docstring(model, tokenizer, async_code))\n",
        "print(generate_verified_docstring(model, tokenizer, chatgpt))"
      ],
      "metadata": {
        "id": "ZSvWmKrmsibX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDlAddXUxmop"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}